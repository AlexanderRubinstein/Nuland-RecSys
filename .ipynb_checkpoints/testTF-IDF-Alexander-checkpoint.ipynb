{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Alexander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_idf = pd.read_json(\"data/stackoverflow-data-idf.json\", lines=True)\n",
    "df_idf = pd.read_json(\"data/jsons/database1.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Alexander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_net_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be more sophisticated\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text = re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    lemmatized_text = \"\"\n",
    "    for word in text_words:\n",
    "        lemma = word_net_lemmatizer.lemmatize(word)\n",
    "        lemmatized_text += \" \" + lemma\n",
    "    text = lemmatized_text[1:] \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf['text'] = df_idf['body']\n",
    "# df_idf['text'] = df_idf['title'] + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x: pre_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need a videographer to remove the backstage photo shoot dance video and maybe a vlog would love to have the camera removed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(stop_words_file):\n",
    "    with open(stop_words_file, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_set = (m.strip() for m in stop_words)\n",
    "        return frozenset(stop_set)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y', 'how', 'your', 'down', 'into', 'above', 'wasn', \"that'll\", \"don't\", 'with', 'will', 'shan', 'yourselves', 'any', 'wouldn', \"won't\", 'just', 'being', 'an', 'hasn', \"didn't\", 'off', \"aren't\", 'o', 'then', 'am', 'below', 'more', 'ourselves', 'that', 'be', 'isn', 'nor', 'were', 'such', 'itself', 'for', 'these', 'been', \"doesn't\", \"wouldn't\", \"shouldn't\", 'his', 'in', 'mustn', 'themselves', 'my', 'its', 'here', 'than', 'at', 'after', 'over', 'himself', 'most', 'what', 'of', \"hadn't\", 'now', 'doing', 'weren', 'but', 'under', 'hers', 'not', 'is', 'our', \"wasn't\", 'until', 'once', 'about', 'as', 'ours', 'm', 'herself', 'and', 'all', 'out', \"you're\", 'some', 'other', 'can', 'no', 'couldn', \"mightn't\", 'this', \"couldn't\", 'him', 'we', 'they', 'there', 'if', 's', 'her', 'didn', 'a', 'both', 'he', 'you', 'so', \"mustn't\", 'she', 'did', 'hadn', 'the', 'on', 'shouldn', 'are', 'aren', \"you'll\", 've', 'needn', \"it's\", 'won', 'i', 'does', 'where', 'up', 'theirs', 'should', 'or', 'll', 'to', 'before', \"needn't\", 're', 'each', \"should've\", 'same', 'again', 't', 'why', 'very', 'mightn', 'against', 'ma', 'when', 'who', 'own', 'do', 'don', 'them', 'only', 'their', \"isn't\", 'yourself', 'while', 'haven', \"you've\", 'through', 'me', 'doesn', \"hasn't\", 'had', 'have', 'by', 'd', 'yours', 'few', 'it', 'those', 'which', 'whom', 'between', \"you'd\", \"haven't\", 'myself', 'because', 'during', 'too', \"weren't\", 'has', 'having', 'ain', 'was', 'from', 'further', \"shan't\", \"she's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alexander\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords = get_stopwords(\"data/stopwords/stopwords.txt\")  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = df_idf['text'].tolist()\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=stop_words, max_features=10000)\n",
    "\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 7138)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wedding',\n",
       " 'photographer',\n",
       " 'need',\n",
       " 'february',\n",
       " 'hour',\n",
       " 'charge',\n",
       " 'bride',\n",
       " 'check',\n",
       " 'south',\n",
       " 'west']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_COO(COO_matrix):\n",
    "    tuples = zip(COO_matrix.col, COO_matrix.data)\n",
    "    return sorted(tuples, key=lambda x:(x[1], x[0]), reverse=True)\n",
    "\n",
    "def topn(sorted_tuples, feature_names, topn):\n",
    "    feature_list = []\n",
    "    value_list = []\n",
    "    sorted_tuples = sorted_tuples[:topn]\n",
    "    for idx, value in sorted_tuples:\n",
    "        feature_list.append(feature_names[idx])\n",
    "        value_list.append(value)\n",
    "    results = {}\n",
    "    for idx in range(len(feature_list)):\n",
    "        results[feature_list[idx]] = value_list[idx]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wedding photographer need a photographer for a wedding february from to or hour charge the bride check the south west of moscow'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_0 = tfidf_transformer.transform(cv.transform([docs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    "sorted_tuples = sort_COO(tfidf_vector_0.tocoo())\n",
    "keywords = topn(sorted_tuples, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "west 0.408918139910497\n",
      "wedding 0.39445350703255283\n",
      "charge 0.382671737159973\n",
      "south 0.36147329378014714\n",
      "check 0.31765348550026157\n",
      "february 0.3009251614760709\n",
      "bride 0.2997670593120834\n",
      "moscow 0.2146019947571679\n",
      "photographer 0.20567555262653536\n",
      "hour 0.15589967180316164\n"
     ]
    }
   ],
   "source": [
    "for k in keywords:\n",
    "    print(k, keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_1 = tfidf_transformer.transform(cv.transform([docs[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cossim = cosine_similarity(tfidf_vector_0, tfidf_vector_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
